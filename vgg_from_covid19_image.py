# -*- coding: utf-8 -*-
"""VGG from covid19-image

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1VRaFhInoEK4LkxKjNz3WwIpb-4AnT9qC
"""

# Commented out IPython magic to ensure Python compatibility.
import os
import tensorflow as tf
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras import layers
from tensorflow.keras import Model
import matplotlib.pyplot as plt
import cv2
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from tensorflow.keras import Model, Input, regularizers
from tensorflow.keras.layers import Dense, Conv2D, MaxPool2D, UpSampling2D
from tensorflow.keras.callbacks import EarlyStopping
from keras.preprocessing import image
import glob
from tqdm import tqdm
import warnings;
warnings.filterwarnings('ignore')
import pathlib
from tqdm import tqdm
# %matplotlib inline
import pandas as pd
from sklearn.utils.multiclass import unique_labels
import matplotlib.image as mpimg
import seaborn as sns
import itertools
from sklearn.metrics import confusion_matrix
from keras.preprocessing.image import ImageDataGenerator
from keras.optimizers import SGD,Adam
from keras.callbacks import ReduceLROnPlateau
from keras.layers import Flatten,Dense,BatchNormalization,Activation,Dropout
from keras.utils import to_categorical

from google.colab import drive
drive.mount('/content/gdrive')

!pip install opendatasets --upgrade --quiet

for dirname, _, filenames in os.walk('/content/gdrive/MyDrive/Covid19-dataset'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

train_dir = pathlib.Path('/content/gdrive/MyDrive/Covid19-dataset/train')
test_dir = pathlib.Path('/content/gdrive/MyDrive/Covid19-dataset/test')

train_count = len(list(train_dir.glob('*/*.*')))
test_count = len(list(test_dir.glob('*/*.*')))

print("Images in Training set : {} \nImages in Test det : {}".format(train_count,test_count))

train_covid_dir = os.path.join(train_dir, 'Covid')
train_normal_dir = os.path.join(train_dir, 'Normal')
train_pneumonia_dir = os.path.join(train_dir, 'Viral Pneumonia')

test_covid_dir = os.path.join(test_dir, 'Covid')
test_normal_dir = os.path.join(test_dir, 'Normal')
test_pneumonia_dir = os.path.join(test_dir, 'Viral Pneumonia')

train_covid_fnames = os.listdir(train_covid_dir)
train_normal_fnames = os.listdir(train_normal_dir)
train_pneumonia_fnames = os.listdir(train_pneumonia_dir)

print(train_covid_fnames[:10])
print(train_normal_fnames[:10])
print(train_pneumonia_fnames[:10])

"""# Preparing the Dataset"""

# Commented out IPython magic to ensure Python compatibility.
# The following code will let us check if the images have been loaded correctly
# %matplotlib inline
import matplotlib.image as mpimg
import matplotlib.pyplot as plt

# Parameters for our graph; we'll output images in a 4x4 configuration

nrows = 6
ncols = 4

pic_index = 0 # Index for iterating over images

# Set up matplotlib fig, and size it to fit 4x4 pics
fig = plt.gcf()
fig.set_size_inches(ncols*4, nrows*4)

pic_index+=8

next_covid_pix = [os.path.join(train_covid_dir, fname)
                for fname in train_covid_fnames[ pic_index-8:pic_index]
               ]

next_normal_pix = [os.path.join(train_normal_dir, fname)
                for fname in train_normal_fnames[ pic_index-8:pic_index]
               ]

next_pneumonia_pix = [os.path.join(train_pneumonia_dir, fname)
                for fname in train_pneumonia_fnames[ pic_index-8:pic_index]
               ]

for i, img_path in enumerate(next_covid_pix+next_normal_pix+next_pneumonia_pix):
  # Set up subplot; subplot indices start at 1
  sp = plt.subplot(nrows, ncols, i + 1)
  sp.axis('Off') # Don't show axes (or gridlines)

  image = mpimg.imread(img_path)
  plt.imshow(image)

plt.show()

train_datagen = ImageDataGenerator(rescale = 1./255.)
test_datagen = ImageDataGenerator( rescale = 1.0/255. )

train_generator = train_datagen.flow_from_directory(train_dir, batch_size = 20, class_mode = 'categorical', target_size = (256, 256))
validation_generator = test_datagen.flow_from_directory( test_dir,  batch_size = 20, class_mode = 'categorical', target_size = (256, 256))

"""# Loading the Base Model"""

from tensorflow.keras.applications.vgg16 import VGG16

base_model = VGG16(input_shape = (256, 256, 3), # Shape of our images
include_top = False, # Leave out the last fully connected layer
weights = 'imagenet', classes = 3)

for layer in base_model.layers:
    layer.trainable = False

# Flatten the output layer to 1 dimension
x = layers.Flatten()(base_model.output)

# Add a fully connected layer with 512 hidden units and ReLU activation
x = layers.Dense(512, activation='relu')(x)

# Add a dropout rate of 0.5
x = layers.Dropout(0.5)(x)

# Add a final softmax layer for classification
x = layers.Dense(3, activation='softmax')(x)

model = tf.keras.models.Model(base_model.input, x)

#Model summary
model.summary()

learn_rate=.001
adam=Adam(lr=learn_rate, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)
model.compile(optimizer = adam, loss = 'categorical_crossentropy',metrics = ['acc'])

vgghist = model.fit(train_generator, validation_data = validation_generator, steps_per_epoch = 10, epochs = 25)